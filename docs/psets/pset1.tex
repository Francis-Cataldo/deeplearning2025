\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{amsmath, amsfonts}
\usepackage{hyperref,bbm, graphicx}

\DeclareMathOperator*{\argmin}{arg\,min}

\title{CSCI 1051 Problem Set 1}
\author{} % TODO: Put your name here
\date{\today}

\begin{document}

\maketitle

\subsection*{Submission Instructions}

Please upload your solutions by
\textbf{5pm Friday January 10, 2025.}
\begin{itemize}
\item You are encouraged to discuss ideas
and work with your classmates. However, you
\textbf{must acknowledge} your collaborators
at the top of each solution on which
you collaborated with others 
and you \textbf{must write} your solutions
independently.
\item Your solutions to theory questions must
be written legibly, or typeset in LaTeX or markdown.
If you would like to use LaTeX, you can import the source of this document 
\href{https://www.rtealwitter.com/deeplearning/psets/pset1.tex}{here}
to Overleaf.
\item I recommend that you write your solutions to coding questions in a Jupyter notebook using Google Colab.
\item You should submit your solutions as a \textbf{single PDF} via the assignment on Gradescope.
\end{itemize}

\newpage \section*{Problem 1: Linear Regression}

Consider a $d$-dimensional multivariate linear regression problem with $n$ samples.
Each sample $i$ consists of a data vector $\mathbf{x}^{(i)} \in \mathbb{R}^d$ and a label $y^{(i)} \in \mathbb{R}$.
From these samples, let $\mathbf{X} \in \mathbb{R}^{d \times n}$ be the data matrix where the $i$th row consists of the $i$th data vector.
Similarly, let $\mathbf{y} \in \mathbb{R}^n$ be the target vector where the $i$th entry consists of the $i$th label.

When we solve the multivariate linear regression problem, we find the coefficients

\begin{align}
    \mathbf{w}^* = \argmin_{\mathbf{w} \in \mathbb{R}^d} \| \mathbf{Xw} - \mathbf{y} \|_2^2.
\end{align}

\subsection*{Part A: Computing the Optimal Solution}

Using the strategy described in class, show that

\begin{align}
    \mathbf{w}^* = (\mathbf{X^\top X})^{-1} \mathbf{X^\top y}.
\end{align}


\subsection*{Part B: The Optimal Solution on Real Data}

Load a real regression dataset and compute the optimal coefficients $\mathbf{w}^*$.

I recommend:
\begin{itemize}
    \item coding in a Colab notebook,
    \item using the scikit-learn regression datasets such as the \href{https://scikit-learn.org/1.5/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes}{diabetes dataset}, and
    \item adapting the example given in the documentation at the above link.
\end{itemize}

\subsection*{Part C: Fast Solution on Real Data}

On the same regression dataset from Part B, use a linear regression solver to compute the optimal coefficients.

I (still) recommend:
\begin{itemize}
    \item coding in a Colab notebook,
    \item using the scikit-learn \href{https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LinearRegression.html}{linear regression solver},
    \item adapting the example given in the documentation at the above link.
\end{itemize}

\noindent
\textbf{Sanity Check}: Ensure that both methods of solving the same regression problem result in the same coefficients.

%\input{solutions/solution1_1}

\end{document}