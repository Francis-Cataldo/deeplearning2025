[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "CSCI 1051: Syllabus",
    "section": "",
    "text": "Course Description: Deep learning has revolutionized our ability to solve complex problems, but its success often obscures the underlying principles that make it work. This course offers a structured approach to understanding and applying deep learning techniques, beginning with the theoretical foundations of linear and logistic regression. We will then generalize these ideas into a three-step deep learning framework (architecture, loss function, and optimizer). Applying this framework, we will explore modern generative methods like auto-regressive language models and diffusion. Because the tools we discuss are so powerful, it is important to understand their limitations. To this end, we will also cover techniques for interpreting deep learning predictions and adding safeguards to generated content.\nPrerequisites: I will assume familiarity with calculus, linear regression, probability, and Python. In particular, I will assume you are comfortable with derivatives, the chain rule, gradients, matrix multiplication, and probability distributions.\nStructure: We will meet on Monday, Tuesday, Wednesday, and Thursday at 75 Shannon in Room 202. The lecture is from 10am to 12pm and the discussion is from 2 to 4pm. I will hold my office hours during the last hour of the discussion. If you would like to meet outside of these times, please email me.\nResources: This class is loosely based on Chinmay Hegde’s phenomenal graduate Deep Learning course at NYU Tandon. For each class, I will post my handwritten slides. I would also like for you to be able to access curated notes. To this end, one of the assignments in the class is to scribe notes on a lecture of your choice.\nDiscussion: Please post all your course related questions on Canvas. If your question reveals your solution to a homework problem, please email me instead.\n\nGrading\nYour grade in the class will be based on the number of points you earn. You will receive an A if you earn 93 or more points, an A- if you earn between 90 and 92 points (inclusive), a B+ if you earn between 87 and 89 points (inclusive), etc.\nParticipation and Questions (13 points): Since winter term classes are smaller, let’s take advantage of the opportunity for more engagement. Unless you have a reasonable excuse (e.g. sickness, family emergency), I expect you to attend every lecture and discussion. Whether you are able to attend or not, I expect you to fill out the form linked from the home page to receive credit for engagement (one point per lecture day that you fill it out). Of course, if you are not able to attend in person, you should watch the recorded zoom lecture before filling out the form.\nProblem Sets (52 points): There will be one problem per class. In order to encourage engagement with the solutions, your problem set grade will be based both on your solutions and your self-grade of your solutions.\nPart 1: Solutions (3 points per homework problem)\nYou may work on the problems with your classmates. However, you should write your solutions and comment your code by yourself.\nPart 2: Self-grade (1 points per homework problem)\nIn order to encourage engagement with the solutions, you will reflect on what you did well in the homework and what you learned. You should submit the self-grade after comparing your answers to the solutions.\nProject (30 points): In order to practice implementing the ideas we cover, you will select a topic we cover in class and implement an algorithm we discussed on a data set of your choosing. You will write a report describing your results and what you learned. You will also give a presentation showcasing your code to the class. You can complete your project as an individual or with a partner.\nCurated Notes (5 points): Notes are a great resource for augmenting your in-class learning. While I have notes for the prior iteration of this course, I since updated the topics in this course to account for much of the progress in deep learning in the last two years. Unfortunately, I haven’t had time to create a new set of notes to reflect the current content. As such, I would like to ask for your help in writing these notes. I will provide an example for the first class (including the finished product and the source). Thereafter, students can sign up on this sheet to write notes for a lecture of your choice. You will have access to my handwritten slides, a zoom recording of the lecture, and any generative model. Because I hope to use these notes in future iterations of the course (with appropriate acknowledgements, of course), I will grade the notes stringently and allow you to resubmit them once.\nLate Policy: I expect all assignments to be turned in on time. If you are unable to turn in an assignment on time, you must email me before the assignment is due to request an extension.\n\n\nHonor Code\nAcademic integrity is an important part of your learning experience. You are welcome to use online material and discuss problems with others but you must explicitly acknowledge the outside resources on the work you submit.\nIf I notice that you have copied someone else’s work without proper attribution (such as code from the internet without a reference link or a solution very close to another student’s without giving credit), I will give you a warning. After the warning, I will subtract 5 points for every violation.\nLarge Language Models: LLMs (that we learn about in class!) are a powerful tool. However, while they are very good at producing human-like text, they have no inherent sense of ‘correctness’. You may use LLMs (as detailed below) but you are wholly responsible for the material you submit.\nYou may use LLMs for:\n\nImplementing short blocks of code that you can easily check.\nSimple questions whose answers you can easily verify.\n\nDo not use LLMS for:\n\nImplementing extensive code or code that you don’t understand.\nComplicated questions (like those on the problem sets) that you would learn from answering yourself.\n\nUltimately, the point of the assignments in this class are for you to practice the concepts. If you use an LLM in lieu of practice, then you deny yourself the chance to learn.\n\n\nAcademic Accommodations\nIf you have a Letter of Accommodation, please contact me as early in the term as possible. If you do not have a Letter of Accommodation and you believe you are eligible, please reach out to the ADA Coordinators at ada@middlebury.edu."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSCI 1051: Deep Learning",
    "section": "",
    "text": "Deep learning from theoretical foundations to state-of-the-art applications.\n\n\n\n\nInstructor: R. Teal Witter. Please call me Teal.\nClass Times: We meet Monday, Tuesday, Wednesday, and Thursday in 75 Shannon St Room 202. The lecture is from 10am to noon and the discussion is from 2 to 4pm.\nOffice Hours: I will hold office hours in 75 Shannon St Room 221 from 3 to 4pm.\nParticipation: I expect you to engage in class, ask questions, and make connections. So that I can get a sense of how you’re doing, please fill out this form once per lecture. (You will receive one point per lecture if you answer the form.)\n\n\nAssignments: You will receive one problem per class. I expect you to solve the problem with your group during the discussion (I’ll be there to answer questions). Once you have solved the problem, you should write up your solution on your own. In addition, there will be a project on a lecture topic of your choice.\n\n\n\nAssignment\n\n\nWork Due\n\n\nSelf-grade Due\n\n\n\n\nProblem Set 1\n\n\nFriday 1/10\n\n\nMonday 1/13\n\n\n\n\nProblem Set 2\n\n\nFriday 1/17\n\n\nMonday 1/20\n\n\n\n\nProblem Set 3\n\n\nFriday 1/24\n\n\nMonday 1/27\n\n\n\n\nProject Proposal\n\n\nMonday 1/20\n\n\n\n\n\n\nProject\n\n\nFriday 1/31\n\n\n\n\n\n\n\n\n\n\nClass\n\n\nTopic\n\n\nSlides\n\n\nResources\n\n\n\n\nThe Three-Step Framework\n\n\n\n\nMonday 1/6\n\n\nLinear Regression and Mean Squared Error\n\n\n\n\n\n\n\n\nTuesday 1/7\n\n\nLogistic Regression and Cross Entropy Loss\n\n\n\n\n\n\n\n\nWednesday 1/8\n\n\nGradient Descent and Neural Networks\n\n\n\n\n\n\n\n\nThursday 1/9\n\n\nBack-propagation and Optimization\n\n\n\n\n\n\n\n\nLanguage Generation\n\n\n\n\nMonday 1/13\n\n\nLanguage Embeddings and Contrastive Loss\n\n\n\n\n\n\n\n\nTuesday 1/14\n\n\nTransformers and Positional Encoding\n\n\n\n\n\n\n\n\nWednesday 1/15\n\n\nLarge Language Models\n\n\n\n\n\n\n\n\nThursday 1/16\n\n\nFinetuning\n\n\n\n\n\n\n\n\nImage Generation\n\n\n\n\nTuesday 1/21\n\n\nConvolutional Neural Networks and Image Embeddings\n\n\n\n\n\n\n\n\nWednesday 1/22\n\n\nDiffusion\n\n\n\n\n\n\n\n\nThursday 1/23\n\n\nSchrödinger Bridges\n\n\n\n\n\n\n\n\nAI Safety\n\n\n\n\nMonday 1/27\n\n\nInterpretability\n\n\n\n\n\n\n\n\nTuesday 1/28\n\n\nWatermarking\n\n\n\n\n\n\n\n\nWednesday 1/29\n\n\nProject Preparation\n\n\n\n\n\n\n\n\nThursday 1/30\n\n\nProject Presentations"
  },
  {
    "objectID": "notes/01_set_size.html",
    "href": "notes/01_set_size.html",
    "title": "Set Size Estimation",
    "section": "",
    "text": "Powered by repeated innovations in chip manufacturing, computers have grown exponentially more powerful over the last several decades. As a result, we have access to unparalleled computational resources and data. For example, a single NASA satellite collects 20 terabytes of satellite images, more than 8 billion searches are made on Google, and estimates suggest the internet creates more than 300 million terabytes of data every single day. Simultaneously, we are quickly approaching the physical limit of how many transistors can be packed on a single chip. In order to learn from the data we have and continue expanding our computational abilities into the future, fast and efficient algorithms are more important than ever.\nAt first glance, an algorithm that performs only a few operations per item in our data set is efficient. However, these algorithms can be too slow when we have lots and lots of data. Instead, we turn to randomized algorithms that can run even faster. Randomized algorithms typically exploit some source of randomness to run on only a small part of the data set (or use only a small amount of space) while still returning an approximately correct result.\nWe can run randomized algorithms in practice to see how well they work. But we also want to prove that they work and understand why. Today, we will solve a problem using randomized algorithms. Before we get to the problems and algorithms, we’ll build some helpful probability tools.\n\n\nConsider a random variable \\(X\\). For example, \\(X\\) could be the outcome of a fair dice roll and be equal to \\(1,2,3,4,5\\) or \\(6\\), each with probability \\(\\frac{1}{6}\\). Formally, we use \\(\\Pr(X=x)\\) to represent the probability that the random variable \\(X\\) is equal to the outcome \\(x\\). The expectation of a discrete random variable is \\[\n\\mathbb{E}[X] = \\sum_{x} x \\Pr(X=x).\n\\] For example, the expected outcome of a fair dice roll is \\(\\mathbb{E}[X] = 1 \\times \\frac{1}{6} + 2 \\times \\frac{1}{6} + 3 \\times \\frac{1}{6} +\n4 \\times \\frac{1}{6} + 5 \\times \\frac{1}{6} + 6 \\times \\frac{1}{6} = \\frac{21}{6}\\). Note: If the random variable is continuous, we can similarly define its expected value using an integral.\nThe expected value tells us where the random variable is on average but we’re also interested in how closely the random variable concentrates around its expectation. The variance of a random variable is \\[\n\\textrm{Var}[X] = \\mathbb{E}\\left[(X - \\mathbb{E}[X])^2\\right].\n\\] Notice that the variance is larger when the random variable is often far from its expectation. In the figure below, can you identify the expected value for each of the three distributions? Which distribution has the largest variance? Which has the smallest?\n\n\n\nThere are a number of useful facts about the expected value and variance. For example,\n\\[\n\\mathbb{E}[\\alpha X] = \\alpha \\mathbb{E}[X]\n\\hspace{1em} \\textrm{and} \\hspace{1em}\n\\textrm{Var}(\\alpha X) = \\alpha^2 \\textrm{Var}(X)\n\\] where \\(\\alpha \\in \\mathbb{R}\\) is a real number. To see this, observe that \\[\n\\mathbb{E}[\\alpha X] = \\sum_{x} \\alpha x \\Pr(X=x)\n= \\alpha \\sum_{x} x \\Pr(X=x) = \\alpha \\mathbb{E}[X]\n\\] and \\[\n\\textrm{Var}(\\alpha X) = \\sum_x (\\alpha x - \\alpha \\mathbb{E}[X])^2 = \\alpha^2 \\sum_x ( x -  \\mathbb{E}[X])^2\n= \\alpha^2 \\textrm{Var}(X).\n\\]\n\n\n\nOnce we have defined random variables, we are often interested in events defined on their outcomes. Let \\(A\\) and \\(B\\) be two events. For example, \\(A\\) could be the event that the dice shows \\(1\\) or \\(2\\) while \\(B\\) could be the event that the dice shows an odd number. We use \\(\\Pr(A \\cap B)\\) to denote the probability that events \\(A\\) and \\(B\\) both happen. Often, we have information about one event and want to see how that changes the probability of another event. We use \\(\\Pr(A | B)\\) to denote the conditional probability of event \\(A\\) given that \\(B\\) happened. We define\n\\[\n\\Pr(A | B) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)}.\n\\]\nIf information about event \\(B\\) does not give us information about event \\(A\\), we say that \\(A\\) and \\(B\\) are independent. Formally, events \\(A\\) and \\(B\\) are independent if \\(\\Pr(A|B) = \\Pr(A)\\). By the definition of conditional probability, an equivalent definition of independence is \\(\\Pr(A \\cap B) = \\Pr(A) \\Pr(B)\\).\nLet’s figure out whether the event \\(A\\) that the dice shows 1 or 2 is independent of the event \\(B\\) that the dice shows an odd number. Well, \\(\\Pr(A \\cap B) = \\frac{1}{6}\\) since the only outcome that satisfy both events is when the dice shows a 1. We also know that \\(\\Pr(A) \\Pr(B) = \\frac{2}{6} \\times \\frac{3}{6} = \\frac{1}{6}\\). So, by the second definition of independence, we can conclude that \\(A\\) and \\(B\\) are independent.\nWe’ve been talking about events defined on random variables, but we’ll also be interested in when random variables are independent. Consider random variables \\(X\\) and \\(Y\\). We say that \\(X\\) and \\(Y\\) are independent if, for all outcomes \\(x\\) and \\(y\\), \\(\\Pr(X=x \\cap Y=y) = \\Pr(X=x) \\Pr(Y=y)\\).\n\n\n\nOne of the most powerful theorems in all of probability is the linearity of expectation.\nTheorem: Let \\(X\\) and \\(Y\\) be random variables. Then \\[\n\\mathbb{E}[X+Y] = \\mathbb{E}[X] + \\mathbb{E}[Y].\n\\] The result is a powerful tool that requires no assumptions on the random variables.\nProof: Observe that \\[\n\\mathbb{E}[X+Y] = \\sum_{x,y}(x+y) \\Pr(X=x \\cap Y=y)\n\\] Now, we’ll separate the equation into two terms and factor out the \\(x\\) and \\(y\\) terms, respectively. \\[\n= \\sum_x x \\sum_y \\Pr(X=x \\cap Y=y)\n+ \\sum_y y \\sum_x \\Pr(X=x \\cap Y=y)\n\\] Finally, using the law of total probability, we have \\[\n= \\sum_x x \\Pr(X=x) + \\sum_y y \\Pr(Y=y) = \\mathbb{E}[X] + \\mathbb{E}[Y].\n\\]\nThere are also several other useful facts about the expected value and variance.\nFact 1: When \\(X\\) and \\(Y\\) are independent, \\(\\mathbb{E}[XY] = \\mathbb{E}[X] \\mathbb{E}[Y]\\).\nProof: Observe that \\[\n\\mathbb{E}[XY] = \\sum_{x,y} xy \\Pr(X=x \\cap Y=y)\n= \\sum_{x,y} xy \\Pr(X=x) \\Pr(Y=y)\n\\]\n\\[\n= \\sum_x x \\Pr(X=x) \\sum_y y \\Pr(Y=y)\n= \\mathbb{E}[X] \\mathbb{E}[Y]\n\\] where the second equality followed by the assumption that \\(X\\) and \\(Y\\) are independent.\nFact 2: Consider a random variable \\(X\\). Then \\(\\textrm{Var}(X) = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\\).\nProof: Observe that \\[\n\\textrm{Var}(X) =\n\\mathbb{E}[(X-\\mathbb{E}[X])^2]\n\\] \\[\n= \\mathbb{E}[X^2 - 2 X \\mathbb{E}[X] + \\mathbb{E}[X]^2]\n= \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\n\\] where the first equality is by definition, the second equality is by foiling, and the third equality is by linearity of expectation and the observation that \\(\\mathbb{E}[X]\\) is a scaler.\nFact 3: When \\(X\\) and \\(Y\\) are independent, \\(\\textrm{Var}(X+Y) = \\textrm{Var}(X) + \\textrm{Var}(Y)\\).\nProof: Observe that\n\\[\\begin{align*}\n\\textrm{Var}(X+Y) &= \\mathbb{E}\\left[(X + Y - \\mathbb{E}[X] - \\mathbb{E}[Y])^2\\right] \\\\\n&= \\mathbb{E}\\left[(X- \\mathbb{E}[X])^2 + 2(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y]) + (Y-\\mathbb{E})^2\\right] \\\\\n&= \\textrm{Var}(X) + 2\\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])]+ \\textrm{Var}(Y).\n\\end{align*}\\] Then, when \\(X\\) and \\(Y\\) are independent, \\[\n\\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])]\n= \\mathbb{E}[XY - \\mathbb{E}[X]Y - X\\mathbb{E}[Y] + \\mathbb{E}[X]\\mathbb{E}[Y]] = 0\n\\] where the last equality follows by Fact 1 when \\(X\\) and \\(Y\\) are independent."
  },
  {
    "objectID": "notes/01_set_size.html#introduction",
    "href": "notes/01_set_size.html#introduction",
    "title": "Set Size Estimation",
    "section": "",
    "text": "Powered by repeated innovations in chip manufacturing, computers have grown exponentially more powerful over the last several decades. As a result, we have access to unparalleled computational resources and data. For example, a single NASA satellite collects 20 terabytes of satellite images, more than 8 billion searches are made on Google, and estimates suggest the internet creates more than 300 million terabytes of data every single day. Simultaneously, we are quickly approaching the physical limit of how many transistors can be packed on a single chip. In order to learn from the data we have and continue expanding our computational abilities into the future, fast and efficient algorithms are more important than ever.\nAt first glance, an algorithm that performs only a few operations per item in our data set is efficient. However, these algorithms can be too slow when we have lots and lots of data. Instead, we turn to randomized algorithms that can run even faster. Randomized algorithms typically exploit some source of randomness to run on only a small part of the data set (or use only a small amount of space) while still returning an approximately correct result.\nWe can run randomized algorithms in practice to see how well they work. But we also want to prove that they work and understand why. Today, we will solve a problem using randomized algorithms. Before we get to the problems and algorithms, we’ll build some helpful probability tools.\n\n\nConsider a random variable \\(X\\). For example, \\(X\\) could be the outcome of a fair dice roll and be equal to \\(1,2,3,4,5\\) or \\(6\\), each with probability \\(\\frac{1}{6}\\). Formally, we use \\(\\Pr(X=x)\\) to represent the probability that the random variable \\(X\\) is equal to the outcome \\(x\\). The expectation of a discrete random variable is \\[\n\\mathbb{E}[X] = \\sum_{x} x \\Pr(X=x).\n\\] For example, the expected outcome of a fair dice roll is \\(\\mathbb{E}[X] = 1 \\times \\frac{1}{6} + 2 \\times \\frac{1}{6} + 3 \\times \\frac{1}{6} +\n4 \\times \\frac{1}{6} + 5 \\times \\frac{1}{6} + 6 \\times \\frac{1}{6} = \\frac{21}{6}\\). Note: If the random variable is continuous, we can similarly define its expected value using an integral.\nThe expected value tells us where the random variable is on average but we’re also interested in how closely the random variable concentrates around its expectation. The variance of a random variable is \\[\n\\textrm{Var}[X] = \\mathbb{E}\\left[(X - \\mathbb{E}[X])^2\\right].\n\\] Notice that the variance is larger when the random variable is often far from its expectation. In the figure below, can you identify the expected value for each of the three distributions? Which distribution has the largest variance? Which has the smallest?\n\n\n\nThere are a number of useful facts about the expected value and variance. For example,\n\\[\n\\mathbb{E}[\\alpha X] = \\alpha \\mathbb{E}[X]\n\\hspace{1em} \\textrm{and} \\hspace{1em}\n\\textrm{Var}(\\alpha X) = \\alpha^2 \\textrm{Var}(X)\n\\] where \\(\\alpha \\in \\mathbb{R}\\) is a real number. To see this, observe that \\[\n\\mathbb{E}[\\alpha X] = \\sum_{x} \\alpha x \\Pr(X=x)\n= \\alpha \\sum_{x} x \\Pr(X=x) = \\alpha \\mathbb{E}[X]\n\\] and \\[\n\\textrm{Var}(\\alpha X) = \\sum_x (\\alpha x - \\alpha \\mathbb{E}[X])^2 = \\alpha^2 \\sum_x ( x -  \\mathbb{E}[X])^2\n= \\alpha^2 \\textrm{Var}(X).\n\\]\n\n\n\nOnce we have defined random variables, we are often interested in events defined on their outcomes. Let \\(A\\) and \\(B\\) be two events. For example, \\(A\\) could be the event that the dice shows \\(1\\) or \\(2\\) while \\(B\\) could be the event that the dice shows an odd number. We use \\(\\Pr(A \\cap B)\\) to denote the probability that events \\(A\\) and \\(B\\) both happen. Often, we have information about one event and want to see how that changes the probability of another event. We use \\(\\Pr(A | B)\\) to denote the conditional probability of event \\(A\\) given that \\(B\\) happened. We define\n\\[\n\\Pr(A | B) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)}.\n\\]\nIf information about event \\(B\\) does not give us information about event \\(A\\), we say that \\(A\\) and \\(B\\) are independent. Formally, events \\(A\\) and \\(B\\) are independent if \\(\\Pr(A|B) = \\Pr(A)\\). By the definition of conditional probability, an equivalent definition of independence is \\(\\Pr(A \\cap B) = \\Pr(A) \\Pr(B)\\).\nLet’s figure out whether the event \\(A\\) that the dice shows 1 or 2 is independent of the event \\(B\\) that the dice shows an odd number. Well, \\(\\Pr(A \\cap B) = \\frac{1}{6}\\) since the only outcome that satisfy both events is when the dice shows a 1. We also know that \\(\\Pr(A) \\Pr(B) = \\frac{2}{6} \\times \\frac{3}{6} = \\frac{1}{6}\\). So, by the second definition of independence, we can conclude that \\(A\\) and \\(B\\) are independent.\nWe’ve been talking about events defined on random variables, but we’ll also be interested in when random variables are independent. Consider random variables \\(X\\) and \\(Y\\). We say that \\(X\\) and \\(Y\\) are independent if, for all outcomes \\(x\\) and \\(y\\), \\(\\Pr(X=x \\cap Y=y) = \\Pr(X=x) \\Pr(Y=y)\\).\n\n\n\nOne of the most powerful theorems in all of probability is the linearity of expectation.\nTheorem: Let \\(X\\) and \\(Y\\) be random variables. Then \\[\n\\mathbb{E}[X+Y] = \\mathbb{E}[X] + \\mathbb{E}[Y].\n\\] The result is a powerful tool that requires no assumptions on the random variables.\nProof: Observe that \\[\n\\mathbb{E}[X+Y] = \\sum_{x,y}(x+y) \\Pr(X=x \\cap Y=y)\n\\] Now, we’ll separate the equation into two terms and factor out the \\(x\\) and \\(y\\) terms, respectively. \\[\n= \\sum_x x \\sum_y \\Pr(X=x \\cap Y=y)\n+ \\sum_y y \\sum_x \\Pr(X=x \\cap Y=y)\n\\] Finally, using the law of total probability, we have \\[\n= \\sum_x x \\Pr(X=x) + \\sum_y y \\Pr(Y=y) = \\mathbb{E}[X] + \\mathbb{E}[Y].\n\\]\nThere are also several other useful facts about the expected value and variance.\nFact 1: When \\(X\\) and \\(Y\\) are independent, \\(\\mathbb{E}[XY] = \\mathbb{E}[X] \\mathbb{E}[Y]\\).\nProof: Observe that \\[\n\\mathbb{E}[XY] = \\sum_{x,y} xy \\Pr(X=x \\cap Y=y)\n= \\sum_{x,y} xy \\Pr(X=x) \\Pr(Y=y)\n\\]\n\\[\n= \\sum_x x \\Pr(X=x) \\sum_y y \\Pr(Y=y)\n= \\mathbb{E}[X] \\mathbb{E}[Y]\n\\] where the second equality followed by the assumption that \\(X\\) and \\(Y\\) are independent.\nFact 2: Consider a random variable \\(X\\). Then \\(\\textrm{Var}(X) = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\\).\nProof: Observe that \\[\n\\textrm{Var}(X) =\n\\mathbb{E}[(X-\\mathbb{E}[X])^2]\n\\] \\[\n= \\mathbb{E}[X^2 - 2 X \\mathbb{E}[X] + \\mathbb{E}[X]^2]\n= \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\n\\] where the first equality is by definition, the second equality is by foiling, and the third equality is by linearity of expectation and the observation that \\(\\mathbb{E}[X]\\) is a scaler.\nFact 3: When \\(X\\) and \\(Y\\) are independent, \\(\\textrm{Var}(X+Y) = \\textrm{Var}(X) + \\textrm{Var}(Y)\\).\nProof: Observe that\n\\[\\begin{align*}\n\\textrm{Var}(X+Y) &= \\mathbb{E}\\left[(X + Y - \\mathbb{E}[X] - \\mathbb{E}[Y])^2\\right] \\\\\n&= \\mathbb{E}\\left[(X- \\mathbb{E}[X])^2 + 2(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y]) + (Y-\\mathbb{E})^2\\right] \\\\\n&= \\textrm{Var}(X) + 2\\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])]+ \\textrm{Var}(Y).\n\\end{align*}\\] Then, when \\(X\\) and \\(Y\\) are independent, \\[\n\\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])]\n= \\mathbb{E}[XY - \\mathbb{E}[X]Y - X\\mathbb{E}[Y] + \\mathbb{E}[X]\\mathbb{E}[Y]] = 0\n\\] where the last equality follows by Fact 1 when \\(X\\) and \\(Y\\) are independent."
  },
  {
    "objectID": "notes/01_set_size.html#set-size-estimation",
    "href": "notes/01_set_size.html#set-size-estimation",
    "title": "Set Size Estimation",
    "section": "Set Size Estimation",
    "text": "Set Size Estimation\nWe’ll pose a problem that has applications in ecology, social networks, and internet indexing. However, while efficiently solving the problem is useful, our purpose is really to gain familiarity with linearity of expectation and learn Markov’s inequality.\nSuppose you run a website that is considering contracting with a company to provide CAPTCHAs for login verification. The company claims to have a database with \\(n=1000000\\) unique CAPTCHAs. For each API call, they’ll return a CAPTCHA chosen uniformly at random from their database. Here’s our problem: How many queries \\(m\\) do we need to make to their API until we can independently verify that they do in fact have a million CAPTCHAs?\nAn obvious approach is to keep calling ther API until we find a million unique CAPTCHAs. Of course, the issue is that we have to make at least a million API calls. That’s not so good if we care about efficiency, they charge us per call, or the size they claim to have in their database is much bigger than a million.\nA more clever approach is to call their API and count duplicates. Intuitively, the larger their database, the fewer duplicates we expect to see. Define a random variable \\(D_{i,j}\\) which is 1 if the \\(i\\)th and \\(j\\)th calls return the same CAPTCHA and 0 otherwise. (To avoid double counting, we’ll assume \\(i &lt; j\\).) For example, in the figure below, the \\(5\\)th, \\(6\\)th, and \\(7\\)th calls returned the same CAPTCHA so \\(D_{5,6}\\), \\(D_{5,7}\\), and \\(D_{6,7}\\) are all 1.\n\n\n\nWhen a random variable can only be 0 or 1, we call it an indicator random variable. Indicator random variables have the special property that their expected value is the probability they are 1. We can define the total number of duplicates \\(D\\) in terms of our indicator random variables \\(D_{i,j}\\).\n\\[\nD = \\sum_{\\substack{i, j \\in \\{1, \\ldots, m\\} \\\\ i &lt; j }} D_{i,j}\n\\]\nWe can calculate the expected number of duplicates using linearity of expectation.\n\\[\n\\mathbb{E}[D] = \\sum_{\\substack{i, j \\in \\{1, \\ldots, m\\} \\\\ i &lt; j }} \\mathbb{E}[D_{i,j}]\n\\]\nSince \\(D_{i,j}\\) is an indicator random variable, we know \\(\\mathbb{E}[D_{i,j}]\\) is the probability the \\(i\\)th and \\(j\\)th CAPTCHA are the same. Since each API call is a uniform and independent sample from the database, the probability the \\(j\\)th CAPTCHA is the same as the \\(i\\)th is \\(\\frac{1}{n}\\). With this observation in hand,\n\\[\n\\mathbb{E}[D] = \\sum_{\\substack{i, j \\in \\{1, \\ldots, m\\} \\\\ i &lt; j }} \\frac{1}{n}\n= \\binom{m}{2} \\frac{1}{n} = \\frac{m(m-1)}{2n}.\n\\]\nSuppose we take \\(m=1000\\) queries and see \\(D=10\\) duplicates. How does this compare to what we would expect if the database had \\(n=1000000\\) CAPTCHAs?\nWell, the expectation would be \\(\\mathbb{E}[D] = \\frac{1000 \\times 999}{2 \\times 1000000} = .4995\\). Something seems wrong… we observed many more duplicates than we expect. Can we formalize this intuition?\n\nMarkov’s Inequality\nConcentration inequalities are a powerful tool in the analysis of randomized algorithms. They tell us how likely it is that a random variable differs from its expectation.\nThere are many concentration inequalities. Some apply in general and some apply only under special assumptions. The concentration inequalities that apply only under special assumptions tend to give stronger results. We’ll start with one of the most simple and general concentration inequalities.\nTheorem: For any non-negative random variable \\(X\\) and any positive threshold \\(t\\), \\[\n\\Pr(X \\geq t) \\leq \\frac{\\mathbb{E}[X]}{t}.\n\\]\nProof: We’ll prove the inequality directly. By the definition of expectation, we have \\[\n\\mathbb{E}[X] = \\sum_{x} x \\Pr(X=x)\n= \\sum_{\\substack{x \\\\ x \\geq t}} x \\Pr(X=x) +\n\\sum_{\\substack{x \\\\ x &lt; t}} x \\Pr(X=x)\n\\] \\[\n\\geq \\sum_{\\substack{x \\\\ x \\geq t}} t \\Pr(X=x) + 0\n= t \\Pr(X \\geq t).\n\\] Rearranging the above inequality gives Markov’s. Can you see where we used that all outcomes \\(x\\) are non-negative?\nNow let’s apply Markov’s inequality to our set size estimation problem. Since the number of duplicates \\(D\\) is always positive, we satisfy the assumption of the inequality. \\[\n\\Pr(D \\geq 10 ) \\leq \\frac{\\mathbb{E}[D]}{10} = \\frac{.4995}{10} = .04995\n\\] The probability of observing the 10 duplicates is less than \\(5\\%\\)! We should probably start asking the CAPTCHA company some questions.\nIn practice, many of the set size estimation problems are slightly different. Instead of checking a claim about the set size, we want to estimate the set size directly. Notice that we computed \\(\\mathbb{E}[D] = \\frac{m(m-1)}{2n}\\). Rearranging, we see that \\(n = \\frac{m(m-1)}{2\\mathbb{E}[D]}\\). Given \\(m\\) samples, we can naturally build an estimator for the whole set size using the empirical number of duplicates we found in the sample. With a little more work, we can show the following.\nClaim: If we make \\(m \\geq c \\frac{\\sqrt{n}}{\\epsilon}\\) samples for a particular constant \\(c\\), then the estimate \\(\\hat{n} = \\frac{m(m-1)}{2D}\\) satisfies \\((1-\\epsilon) n \\leq \\hat{n} \\leq (1+\\epsilon) n\\) with probability \\(9/10\\)."
  },
  {
    "objectID": "notes/01_regression.html",
    "href": "notes/01_regression.html",
    "title": "Linear Regression and Mean Squared Error",
    "section": "",
    "text": "When I first heard about ‘’machine learning’’, I imagined a machine that was rewarded every time it gave the right answer. Maybe there were electric carrots and sticks that no one had bothered to tell me about? While I now know about as little as I did then about computer hardware, I have learned machine learning is fundamentally a mathematical process.\nThe truth is that all the ‘magic’ lies in optimization and math. Luckily, you’ve been learning about these very ideas for years! We’ll review the concepts and then jump in to machine learning through the example of linear regression.\n\n\nImagine a function \\(\\ell: \\mathbb{R} \\to \\mathbb{R}\\). This notation means it takes a single real number as input and outputs a single real number. In general, we should be careful about whether we can even differentiate a function but, we’re computer scientists so we’ll just risk it for the biscuit.\nThe derivative of \\(\\ell\\) with respect to its input \\(z\\) we’ll denote by \\(\\frac{\\partial}{\\partial z}[\\ell(z)]\\).\nFormally, the derivative is defined as \\[\n\\frac{\\partial}{\\partial z}[\\ell(z)]\n= \\lim_{h \\to 0} \\frac{\\ell(z + h) - \\ell(z)}{h}.\n\\] This is just the slope of a line and you’ve been learning about it for ages.\nFor example, we know that for \\(\\ell(z) = z^a + b\\), the derivative \\(\\frac{\\partial}{\\partial z}[\\ell(z)] = a z^{a-1}\\) by the power rule.\nWe also know more fancy rules like that for \\(\\ell(z) = \\ln (z)\\), the derivative \\(\\frac{\\partial}{\\partial z}[\\ell(z)] = \\frac1{z}\\).\n\n\n\nWhile the basic operations are nice, we’re not always so lucky. Modern machine learning chains many many complicated functions together. Fortunately, we will think of these functions modularly.\nLet \\(g: \\mathbb{R} \\to \\mathbb{R}\\) be another function. Consider the compositive function \\(g(\\ell(z))\\).\nBy the chain rule, the derivative \\[\n\\frac{\\partial }{\\partial z}[g(\\ell(z))]\n= \\frac{\\partial g}{\\partial z}(\\ell(z))\n\\frac{\\partial}{\\partial z}[\\ell(z)].\n\\]\nOften, we will also multiply functions together. The product rule tells us that \\[\n\\frac{\\partial }{\\partial z}[g(z) \\ell(z)]\n= g(z) \\frac{\\partial}{\\partial z}[\\ell(z)]\n+ \\ell(z) \\frac{\\partial}{\\partial z}[g(z)].\n\\]\n\n\n\nIn machine learning, we process lots of data. So the functions we consider generally have multivariate input. Consider \\(\\ell: \\mathbb{R}^d \\to \\mathbb{R}\\). Now, the output of the function is still a real number but the input consists of \\(d\\) real numbers.\nWith multivariate functions, we will talk about the partial derivative with respect to each one of the inputs \\(z_1, \\ldots, z_d\\). We will use the vector \\(\\mathbf{z} \\in \\mathbb{R}^d\\) to represent all \\(d\\) inputs. The partial derivative with respect to \\(z_i\\) is denoted by \\(\\frac{\\partial}{\\partial z_i}[\\ell(\\mathbf{z})]\\) and treats all the variables \\(z_j\\) for \\(j \\neq i\\) as constant.\nThe gradient of \\(\\ell\\) with respect to the input \\(\\mathbf{z}\\) is the vector \\(\\nabla_{\\mathbf{z}} \\ell\\). The \\(i\\)th entry of this vector is given by the partial derivative of \\(\\ell\\) with respect to \\(z_i\\). In mathematical notation, \\[\n\\nabla_\\mathbf{z} \\ell = \\begin{cases} \\frac{\\partial}{\\partial z_1}[\\ell(\\mathbf{z})] \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial z_d}[\\ell(\\mathbf{z})] \\\\ \\end{cases}\n\\]\nJust like the derivative in one dimension, the gradient gives contains information about the slope of \\(\\ell\\) with respect to each of the \\(d\\) dimensions.\n\n\n\nConsider two vectors \\(\\mathbf{u} \\in \\mathbb{R}^d\\) and \\(\\mathbf{v} \\in \\mathbb{R}^d\\). We will use \\(\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^d u_i v_i\\) to denote the inner product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). The \\(\\ell_2\\)-norm of \\(v\\) is given by \\(\\|\\mathbf{v}\\|_2 = \\sqrt{\\mathbf{u} \\cdot \\mathbf{u}}\\).\nMatrix multiplication lives at the heart of deep learning. In fact, deep learning really started to take off when researchers realized that the Graphical Processing Unit (GPU) could be used to perform gradient updates in addition to the matrix multiplication it was designed to do for gaming.\nConsider two matrices: \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times m}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{m \\times n}\\) where \\(d \\neq n\\).\nWe can only multiply two matrices when their inner dimension agrees. Because the number of columns in \\(\\mathbf{A}\\) is the same as the number of rows in \\(\\mathbf{B}\\), we can compute \\(\\mathbf{AB}\\). However, because the number of columns in \\(\\mathbf{B}\\) is not the same as the number of rows in \\(\\mathbf{A}\\), the product is \\(\\mathbf{BA}\\) is not defined.\nWhen we can multiply two matrices, the \\((i,j)\\) entry in \\(\\mathbf{AB}\\) is given by the inner product between the \\(i\\)th row of \\(\\mathbf{A}\\) and the \\(j\\)th column of \\(\\mathbf{B}\\). The resulting dimensions of the matrix product will be the number of rows in \\(\\mathbf{A}\\) by the number of columns in \\(\\mathbf{B}\\).\n\n\n\nIf we have a scalar equation \\(ax = b\\), we can simply solve for \\(x\\) by dividing both sides by \\(a\\). In effect, we are applying the inverse of \\(a\\) to \\(a\\) since \\(\\frac1{a} a =1\\). The same principle applies to matrices. For matrices, the \\(n \\times n\\) identity matrix generalizes the scalar \\(1\\). The identity matrix is denoted by \\(\\mathbf{I}_{n \\times n} \\in \\mathbb{R}^{n \\times n}\\): the on-diagonal entries \\((i,i)\\) are 1 while the off-diagonal entries \\((i,j)\\) for \\(i\\neq j\\) are 0.\nSuppose we have the equation \\(\\mathbf{Ax} = \\mathbf{b}\\) where \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times d}\\), \\(\\mathbf{x} \\in \\mathbb{R}^d\\), and \\(\\mathbf{b} \\in \\mathbb{R}^d\\). (Notice that the inner dimensions of \\(\\mathbf{A}\\) and \\(\\mathbf{x}\\) agree so their multiplication is well-defined, and the resulting vector is the same dimension as \\(\\mathbf{b}\\).)\nIf we want to solve for \\(\\mathbf{x}\\), we can use the same inverse idea. For a matrix \\(\\mathbf{A}\\), we use \\(\\mathbf{A}^{-1}\\) to denote its inverse. The inverse is defined so that \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\) where \\(\\mathbf{I}_{n \\times n}\\) is the identity matrix."
  },
  {
    "objectID": "notes/01_regression.html#introduction",
    "href": "notes/01_regression.html#introduction",
    "title": "Linear Regression and Mean Squared Error",
    "section": "Introduction",
    "text": "Introduction\nPowered by repeated innovations in chip manufacturing, computers have grown exponentially more powerful over the last several decades. As a result, we have access to unparalleled computational resources and data. For example, a single NASA satellite collects 20 terabytes of satellite images, more than 8 billion searches are made on Google, and estimates suggest the internet creates more than 300 million terabytes of data every single day. Simultaneously, we are quickly approaching the physical limit of how many transistors can be packed on a single chip. In order to learn from the data we have and continue expanding our computational abilities into the future, fast and efficient algorithms are more important than ever.\nAt first glance, an algorithm that performs only a few operations per item in our data set is efficient. However, these algorithms can be too slow when we have lots and lots of data. Instead, we turn to randomized algorithms that can run even faster. Randomized algorithms typically exploit some source of randomness to run on only a small part of the data set (or use only a small amount of space) while still returning an approximately correct result.\nWe can run randomized algorithms in practice to see how well they work. But we also want to prove that they work and understand why. Today, we will solve a problem using randomized algorithms. Before we get to the problems and algorithms, we’ll build some helpful probability tools.\n\nProbability Background\nConsider a random variable \\(X\\). For example, \\(X\\) could be the outcome of a fair dice roll and be equal to \\(1,2,3,4,5\\) or \\(6\\), each with probability \\(\\frac{1}{6}\\). Formally, we use \\(\\Pr(X=x)\\) to represent the probability that the random variable \\(X\\) is equal to the outcome \\(x\\). The expectation of a discrete random variable is \\[\n\\mathbb{E}[X] = \\sum_{x} x \\Pr(X=x).\n\\] For example, the expected outcome of a fair dice roll is \\(\\mathbb{E}[X] = 1 \\times \\frac{1}{6} + 2 \\times \\frac{1}{6} + 3 \\times \\frac{1}{6} +\n4 \\times \\frac{1}{6} + 5 \\times \\frac{1}{6} + 6 \\times \\frac{1}{6} = \\frac{21}{6}\\). Note: If the random variable is continuous, we can similarly define its expected value using an integral.\nThe expected value tells us where the random variable is on average but we’re also interested in how closely the random variable concentrates around its expectation. The variance of a random variable is \\[\n\\textrm{Var}[X] = \\mathbb{E}\\left[(X - \\mathbb{E}[X])^2\\right].\n\\] Notice that the variance is larger when the random variable is often far from its expectation. In the figure below, can you identify the expected value for each of the three distributions? Which distribution has the largest variance? Which has the smallest?\n\n\n\nThere are a number of useful facts about the expected value and variance. For example,\n\\[\n\\mathbb{E}[\\alpha X] = \\alpha \\mathbb{E}[X]\n\\hspace{1em} \\textrm{and} \\hspace{1em}\n\\textrm{Var}(\\alpha X) = \\alpha^2 \\textrm{Var}(X)\n\\] where \\(\\alpha \\in \\mathbb{R}\\) is a real number. To see this, observe that \\[\n\\mathbb{E}[\\alpha X] = \\sum_{x} \\alpha x \\Pr(X=x)\n= \\alpha \\sum_{x} x \\Pr(X=x) = \\alpha \\mathbb{E}[X]\n\\] and \\[\n\\textrm{Var}(\\alpha X) = \\sum_x (\\alpha x - \\alpha \\mathbb{E}[X])^2 = \\alpha^2 \\sum_x ( x -  \\mathbb{E}[X])^2\n= \\alpha^2 \\textrm{Var}(X).\n\\]\n\n\nIndependent Random Variables\nOnce we have defined random variables, we are often interested in events defined on their outcomes. Let \\(A\\) and \\(B\\) be two events. For example, \\(A\\) could be the event that the dice shows \\(1\\) or \\(2\\) while \\(B\\) could be the event that the dice shows an odd number. We use \\(\\Pr(A \\cap B)\\) to denote the probability that events \\(A\\) and \\(B\\) both happen. Often, we have information about one event and want to see how that changes the probability of another event. We use \\(\\Pr(A | B)\\) to denote the conditional probability of event \\(A\\) given that \\(B\\) happened. We define\n\\[\n\\Pr(A | B) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)}.\n\\]\nIf information about event \\(B\\) does not give us information about event \\(A\\), we say that \\(A\\) and \\(B\\) are independent. Formally, events \\(A\\) and \\(B\\) are independent if \\(\\Pr(A|B) = \\Pr(A)\\). By the definition of conditional probability, an equivalent definition of independence is \\(\\Pr(A \\cap B) = \\Pr(A) \\Pr(B)\\).\nLet’s figure out whether the event \\(A\\) that the dice shows 1 or 2 is independent of the event \\(B\\) that the dice shows an odd number. Well, \\(\\Pr(A \\cap B) = \\frac{1}{6}\\) since the only outcome that satisfy both events is when the dice shows a 1. We also know that \\(\\Pr(A) \\Pr(B) = \\frac{2}{6} \\times \\frac{3}{6} = \\frac{1}{6}\\). So, by the second definition of independence, we can conclude that \\(A\\) and \\(B\\) are independent.\nWe’ve been talking about events defined on random variables, but we’ll also be interested in when random variables are independent. Consider random variables \\(X\\) and \\(Y\\). We say that \\(X\\) and \\(Y\\) are independent if, for all outcomes \\(x\\) and \\(y\\), \\(\\Pr(X=x \\cap Y=y) = \\Pr(X=x) \\Pr(Y=y)\\).\n\n\nLinearity of Expectation\nOne of the most powerful theorems in all of probability is the linearity of expectation.\nTheorem: Let \\(X\\) and \\(Y\\) be random variables. Then \\[\n\\mathbb{E}[X+Y] = \\mathbb{E}[X] + \\mathbb{E}[Y].\n\\] The result is a powerful tool that requires no assumptions on the random variables.\nProof: Observe that \\[\n\\mathbb{E}[X+Y] = \\sum_{x,y}(x+y) \\Pr(X=x \\cap Y=y)\n\\] Now, we’ll separate the equation into two terms and factor out the \\(x\\) and \\(y\\) terms, respectively. \\[\n= \\sum_x x \\sum_y \\Pr(X=x \\cap Y=y)\n+ \\sum_y y \\sum_x \\Pr(X=x \\cap Y=y)\n\\] Finally, using the law of total probability, we have \\[\n= \\sum_x x \\Pr(X=x) + \\sum_y y \\Pr(Y=y) = \\mathbb{E}[X] + \\mathbb{E}[Y].\n\\]\nThere are also several other useful facts about the expected value and variance.\nFact 1: When \\(X\\) and \\(Y\\) are independent, \\(\\mathbb{E}[XY] = \\mathbb{E}[X] \\mathbb{E}[Y]\\).\nProof: Observe that \\[\n\\mathbb{E}[XY] = \\sum_{x,y} xy \\Pr(X=x \\cap Y=y)\n= \\sum_{x,y} xy \\Pr(X=x) \\Pr(Y=y)\n\\]\n\\[\n= \\sum_x x \\Pr(X=x) \\sum_y y \\Pr(Y=y)\n= \\mathbb{E}[X] \\mathbb{E}[Y]\n\\] where the second equality followed by the assumption that \\(X\\) and \\(Y\\) are independent.\nFact 2: Consider a random variable \\(X\\). Then \\(\\textrm{Var}(X) = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\\).\nProof: Observe that \\[\n\\textrm{Var}(X) =\n\\mathbb{E}[(X-\\mathbb{E}[X])^2]\n\\] \\[\n= \\mathbb{E}[X^2 - 2 X \\mathbb{E}[X] + \\mathbb{E}[X]^2]\n= \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\n\\] where the first equality is by definition, the second equality is by foiling, and the third equality is by linearity of expectation and the observation that \\(\\mathbb{E}[X]\\) is a scaler.\nFact 3: When \\(X\\) and \\(Y\\) are independent, \\(\\textrm{Var}(X+Y) = \\textrm{Var}(X) + \\textrm{Var}(Y)\\).\nProof: Observe that\n\\[\\begin{align*}\n\\textrm{Var}(X+Y) &= \\mathbb{E}\\left[(X + Y - \\mathbb{E}[X] - \\mathbb{E}[Y])^2\\right] \\\\\n&= \\mathbb{E}\\left[(X- \\mathbb{E}[X])^2 + 2(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y]) + (Y-\\mathbb{E})^2\\right] \\\\\n&= \\textrm{Var}(X) + 2\\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])]+ \\textrm{Var}(Y).\n\\end{align*}\\] Then, when \\(X\\) and \\(Y\\) are independent, \\[\n\\mathbb{E}[(X-\\mathbb{E}[X])(Y-\\mathbb{E}[Y])]\n= \\mathbb{E}[XY - \\mathbb{E}[X]Y - X\\mathbb{E}[Y] + \\mathbb{E}[X]\\mathbb{E}[Y]] = 0\n\\] where the last equality follows by Fact 1 when \\(X\\) and \\(Y\\) are independent."
  },
  {
    "objectID": "notes/01_regression.html#set-size-estimation",
    "href": "notes/01_regression.html#set-size-estimation",
    "title": "Linear Regression and Mean Squared Error",
    "section": "Set Size Estimation",
    "text": "Set Size Estimation\nWe’ll pose a problem that has applications in ecology, social networks, and internet indexing. However, while efficiently solving the problem is useful, our purpose is really to gain familiarity with linearity of expectation and learn Markov’s inequality.\nSuppose you run a website that is considering contracting with a company to provide CAPTCHAs for login verification. The company claims to have a database with \\(n=1000000\\) unique CAPTCHAs. For each API call, they’ll return a CAPTCHA chosen uniformly at random from their database. Here’s our problem: How many queries \\(m\\) do we need to make to their API until we can independently verify that they do in fact have a million CAPTCHAs?\nAn obvious approach is to keep calling ther API until we find a million unique CAPTCHAs. Of course, the issue is that we have to make at least a million API calls. That’s not so good if we care about efficiency, they charge us per call, or the size they claim to have in their database is much bigger than a million.\nA more clever approach is to call their API and count duplicates. Intuitively, the larger their database, the fewer duplicates we expect to see. Define a random variable \\(D_{i,j}\\) which is 1 if the \\(i\\)th and \\(j\\)th calls return the same CAPTCHA and 0 otherwise. (To avoid double counting, we’ll assume \\(i &lt; j\\).) For example, in the figure below, the \\(5\\)th, \\(6\\)th, and \\(7\\)th calls returned the same CAPTCHA so \\(D_{5,6}\\), \\(D_{5,7}\\), and \\(D_{6,7}\\) are all 1.\n\n\n\nWhen a random variable can only be 0 or 1, we call it an indicator random variable. Indicator random variables have the special property that their expected value is the probability they are 1. We can define the total number of duplicates \\(D\\) in terms of our indicator random variables \\(D_{i,j}\\).\n\\[\nD = \\sum_{\\substack{i, j \\in \\{1, \\ldots, m\\} \\\\ i &lt; j }} D_{i,j}\n\\]\nWe can calculate the expected number of duplicates using linearity of expectation.\n\\[\n\\mathbb{E}[D] = \\sum_{\\substack{i, j \\in \\{1, \\ldots, m\\} \\\\ i &lt; j }} \\mathbb{E}[D_{i,j}]\n\\]\nSince \\(D_{i,j}\\) is an indicator random variable, we know \\(\\mathbb{E}[D_{i,j}]\\) is the probability the \\(i\\)th and \\(j\\)th CAPTCHA are the same. Since each API call is a uniform and independent sample from the database, the probability the \\(j\\)th CAPTCHA is the same as the \\(i\\)th is \\(\\frac{1}{n}\\). With this observation in hand,\n\\[\n\\mathbb{E}[D] = \\sum_{\\substack{i, j \\in \\{1, \\ldots, m\\} \\\\ i &lt; j }} \\frac{1}{n}\n= \\binom{m}{2} \\frac{1}{n} = \\frac{m(m-1)}{2n}.\n\\]\nSuppose we take \\(m=1000\\) queries and see \\(D=10\\) duplicates. How does this compare to what we would expect if the database had \\(n=1000000\\) CAPTCHAs?\nWell, the expectation would be \\(\\mathbb{E}[D] = \\frac{1000 \\times 999}{2 \\times 1000000} = .4995\\). Something seems wrong… we observed many more duplicates than we expect. Can we formalize this intuition?\n\nMarkov’s Inequality\nConcentration inequalities are a powerful tool in the analysis of randomized algorithms. They tell us how likely it is that a random variable differs from its expectation.\nThere are many concentration inequalities. Some apply in general and some apply only under special assumptions. The concentration inequalities that apply only under special assumptions tend to give stronger results. We’ll start with one of the most simple and general concentration inequalities.\nTheorem: For any non-negative random variable \\(X\\) and any positive threshold \\(t\\), \\[\n\\Pr(X \\geq t) \\leq \\frac{\\mathbb{E}[X]}{t}.\n\\]\nProof: We’ll prove the inequality directly. By the definition of expectation, we have \\[\n\\mathbb{E}[X] = \\sum_{x} x \\Pr(X=x)\n= \\sum_{\\substack{x \\\\ x \\geq t}} x \\Pr(X=x) +\n\\sum_{\\substack{x \\\\ x &lt; t}} x \\Pr(X=x)\n\\] \\[\n\\geq \\sum_{\\substack{x \\\\ x \\geq t}} t \\Pr(X=x) + 0\n= t \\Pr(X \\geq t).\n\\] Rearranging the above inequality gives Markov’s. Can you see where we used that all outcomes \\(x\\) are non-negative?\nNow let’s apply Markov’s inequality to our set size estimation problem. Since the number of duplicates \\(D\\) is always positive, we satisfy the assumption of the inequality. \\[\n\\Pr(D \\geq 10 ) \\leq \\frac{\\mathbb{E}[D]}{10} = \\frac{.4995}{10} = .04995\n\\] The probability of observing the 10 duplicates is less than \\(5\\%\\)! We should probably start asking the CAPTCHA company some questions.\nIn practice, many of the set size estimation problems are slightly different. Instead of checking a claim about the set size, we want to estimate the set size directly. Notice that we computed \\(\\mathbb{E}[D] = \\frac{m(m-1)}{2n}\\). Rearranging, we see that \\(n = \\frac{m(m-1)}{2\\mathbb{E}[D]}\\). Given \\(m\\) samples, we can naturally build an estimator for the whole set size using the empirical number of duplicates we found in the sample. With a little more work, we can show the following.\nClaim: If we make \\(m \\geq c \\frac{\\sqrt{n}}{\\epsilon}\\) samples for a particular constant \\(c\\), then the estimate \\(\\hat{n} = \\frac{m(m-1)}{2D}\\) satisfies \\((1-\\epsilon) n \\leq \\hat{n} \\leq (1+\\epsilon) n\\) with probability \\(9/10\\)."
  },
  {
    "objectID": "notes/01_regression.html#linear-regression",
    "href": "notes/01_regression.html#linear-regression",
    "title": "Linear Regression and Mean Squared Error",
    "section": "Linear Regression",
    "text": "Linear Regression\n\nGoal\nWe will begin the course in the supervised learning setting. In this setting, we are given labelled data with input features and an outcome. Formally, we will have \\(n\\) labelled observations \\((x_1, y_1), (x_2, y_2), \\ldots (x_n, y_n)\\). In general, we will have \\(y \\in \\mathbb{R}\\). For simplicity, we will assume for now that \\(x \\in \\mathbb{R}\\).\nOur goal is to process the data and learn a function that approximates the outcomes. In mathematical notation, we want to learn a function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) so that \\(f(x_i) \\approx y_i\\) for the labelled observations.\nBefore we dive into the specific way we will accomplish this with linear regression, let’s discuss the general deep learning framework. This three-step framework gives a flexible scaffolding that we will use to understand almost every topic in this course.\nThe three-Step framework includes:\n• Model: The function that we’ll use to process the input and produce a corresponding output.\n• Loss: The function that measures the quality of the outputs from our model. (Without loss of generality, we will assume that lower is better.)\n• Optimizer: The method of updating the model to improve the loss.\n\n\nLinear Model\nWith these general concepts in mind, we’ll explore linear regression. As its name suggests, linear regression uses a linear model to process the input and approximate the output.\nLet \\(w \\in \\mathbb{R}\\) be a weight parameter. The linear model (for one-dimensional inputs) is given by \\(f(x) = wx\\).\nUnlike many deep learning models, we can visualize the linear model since it is given by a line. In the plot, we have the \\(n=10\\) data points plotted in 2 dimensions. There is one linear model \\(f(x) = 2x\\) that closely approximates the data and another linear model \\(f(x)=\\frac12 x\\) that does not approximate the data.\n\n\n\nOur goal is to learn how to find a linear model that fits the data well. Before we can do this though, we need to figure out how to measure how well the line fits the data.\n\n\nMean Squared Error Loss\nOur goal for the loss function is to measure how closely the data fits the prediction made by our model. Intuitively, we should take the difference between the prediction and the true outcome \\(f(x_i)-y_i\\).\nThe issue with this approach is that \\(f(x_i)-y_i\\) can be small (negative) even when \\(f(x_i) \\neq y_i\\). A natural fix is to take the absolute value \\(|f(x_i) - y_i|\\). The benefit is that the loss is \\(0\\) if and only if \\(f(x_i) = y_i\\). However,\n\\(\\mathcal{L}(w) = \\frac1{n} \\sum_{i=1}^n (f(x_i) - y_i)^2\\)\nMean squared error: differentiable everywhere, penalize errors that are really far, convex\n\n\n\n\n\nExact Optimization\nImagine data is fixed, how to change weights \\(w\\)?\nSo we will think of optimizing \\(\\mathcal{L}: \\mathbb{R} \\to \\mathbb{R}\\)\nLoss is convex, so only one minimum\nSet \\(\\frac{\\partial \\mathcal{L}}{\\partial w}\\) to 0 and solve for \\(w\\)\nWe will use the chain rule and the power rule to compute the derivative of \\(\\mathcal{L}\\)\n\\[\n\\frac{\\partial}{\\partial w}[\\mathcal{L}(w)]\n= \\frac1{n} \\sum_{i=1}^n \\frac{\\partial}{\\partial w} [(f(x_i) - y_i)^2]\n= \\frac1{n} \\sum_{i=1}^n 2(f(x_i) - y_i) \\frac{\\partial}{\\partial w} [(f(x_i) - y_i)]\n= \\frac1{n} \\sum_{i=1}^n 2(w x_i - y_i) x_i\n\\] where the last equality follows because \\(\\frac{\\partial}{\\partial w} wx_i = x_i\\).\nSetting the derivative to \\(0\\) and solving for \\(w\\), we get \\(\\frac2{n} \\sum_{i=1}^n w x_i^2 = \\frac2{n} \\sum_{i=1}^n y_i x_i\\) and so \\[\nw = \\frac{\\sum_{i=1}^n y_i}{\\sum_{i=1}^n x_i^2}.\n\\]"
  },
  {
    "objectID": "notes/01_regression.html#multivariate-linear-regression",
    "href": "notes/01_regression.html#multivariate-linear-regression",
    "title": "Linear Regression and Mean Squared Error",
    "section": "Multivariate Linear Regression",
    "text": "Multivariate Linear Regression\nRecall our setting where we observe \\(n\\) training observations \\((\\mathbf{x_1}, y_1), \\ldots, (\\mathbf{x_n}, y_n)\\). In general, the data we are interested in is high-dimensional so \\(\\mathbf{x}_i \\in \\mathbb{R}^d\\) rather than the prior setting where \\(x_i \\in \\mathbb{R}\\).\n\nLinear Model\nInstead of using a single weight \\(w \\in \\mathbb{R}\\), we will use \\(d\\) weights \\(\\mathbf{w} \\in \\mathbb{R}^d\\). Then the model is given by \\(f(x) = \\mathbf{w} \\cdot \\mathbf{x}\\).\n\n\nMean Squared Error\nSince the output of \\(f\\) is still a single real number, we do not have to change the loss function.\nHowever, we will use our linear algebra notation to write the mean squared error in a fun way.\nLet \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times d}\\) be the data matrix where the \\(i\\)th row is \\(\\mathbf{x}_i^\\top\\). Similarly, let \\(\\mathbf{y} \\in \\mathbf{R}^n\\) be the target vector where the \\(i\\)th entry is \\(y_i\\).\nThen we can write the mean squared error loss as \\(\\mathcal{L}(\\mathbf{w}) = \\frac1{n} \\| \\mathbf{X w - y} \\|_2^2\\).\n\n\nExact Optimization\nJust like computing the derivative and setting it to \\(0\\), we can compute the gradient and set it to \\(\\mathbf{0} \\in \\mathbb{R}^d\\). In mathematical notation, we will set \\(\\nabla_\\mathbf{w} \\mathcal{L}(\\mathbf{w}) = \\mathbf{0}\\) and solve for \\(\\mathbf{w}\\)."
  },
  {
    "objectID": "notes/01_regression.html#math-review",
    "href": "notes/01_regression.html#math-review",
    "title": "Linear Regression and Mean Squared Error",
    "section": "",
    "text": "When I first heard about ‘’machine learning’’, I imagined a machine that was rewarded every time it gave the right answer. Maybe there were electric carrots and sticks that no one had bothered to tell me about? While I now know about as little as I did then about computer hardware, I have learned machine learning is fundamentally a mathematical process.\nThe truth is that all the ‘magic’ lies in optimization and math. Luckily, you’ve been learning about these very ideas for years! We’ll review the concepts and then jump in to machine learning through the example of linear regression.\n\n\nImagine a function \\(\\ell: \\mathbb{R} \\to \\mathbb{R}\\). This notation means it takes a single real number as input and outputs a single real number. In general, we should be careful about whether we can even differentiate a function but, we’re computer scientists so we’ll just risk it for the biscuit.\nThe derivative of \\(\\ell\\) with respect to its input \\(z\\) we’ll denote by \\(\\frac{\\partial}{\\partial z}[\\ell(z)]\\).\nFormally, the derivative is defined as \\[\n\\frac{\\partial}{\\partial z}[\\ell(z)]\n= \\lim_{h \\to 0} \\frac{\\ell(z + h) - \\ell(z)}{h}.\n\\] This is just the slope of a line and you’ve been learning about it for ages.\nFor example, we know that for \\(\\ell(z) = z^a + b\\), the derivative \\(\\frac{\\partial}{\\partial z}[\\ell(z)] = a z^{a-1}\\) by the power rule.\nWe also know more fancy rules like that for \\(\\ell(z) = \\ln (z)\\), the derivative \\(\\frac{\\partial}{\\partial z}[\\ell(z)] = \\frac1{z}\\).\n\n\n\nWhile the basic operations are nice, we’re not always so lucky. Modern machine learning chains many many complicated functions together. Fortunately, we will think of these functions modularly.\nLet \\(g: \\mathbb{R} \\to \\mathbb{R}\\) be another function. Consider the compositive function \\(g(\\ell(z))\\).\nBy the chain rule, the derivative \\[\n\\frac{\\partial }{\\partial z}[g(\\ell(z))]\n= \\frac{\\partial g}{\\partial z}(\\ell(z))\n\\frac{\\partial}{\\partial z}[\\ell(z)].\n\\]\nOften, we will also multiply functions together. The product rule tells us that \\[\n\\frac{\\partial }{\\partial z}[g(z) \\ell(z)]\n= g(z) \\frac{\\partial}{\\partial z}[\\ell(z)]\n+ \\ell(z) \\frac{\\partial}{\\partial z}[g(z)].\n\\]\n\n\n\nIn machine learning, we process lots of data. So the functions we consider generally have multivariate input. Consider \\(\\ell: \\mathbb{R}^d \\to \\mathbb{R}\\). Now, the output of the function is still a real number but the input consists of \\(d\\) real numbers.\nWith multivariate functions, we will talk about the partial derivative with respect to each one of the inputs \\(z_1, \\ldots, z_d\\). We will use the vector \\(\\mathbf{z} \\in \\mathbb{R}^d\\) to represent all \\(d\\) inputs. The partial derivative with respect to \\(z_i\\) is denoted by \\(\\frac{\\partial}{\\partial z_i}[\\ell(\\mathbf{z})]\\) and treats all the variables \\(z_j\\) for \\(j \\neq i\\) as constant.\nThe gradient of \\(\\ell\\) with respect to the input \\(\\mathbf{z}\\) is the vector \\(\\nabla_{\\mathbf{z}} \\ell\\). The \\(i\\)th entry of this vector is given by the partial derivative of \\(\\ell\\) with respect to \\(z_i\\). In mathematical notation, \\[\n\\nabla_\\mathbf{z} \\ell = \\begin{cases} \\frac{\\partial}{\\partial z_1}[\\ell(\\mathbf{z})] \\\\ \\vdots \\\\ \\frac{\\partial}{\\partial z_d}[\\ell(\\mathbf{z})] \\\\ \\end{cases}\n\\]\nJust like the derivative in one dimension, the gradient gives contains information about the slope of \\(\\ell\\) with respect to each of the \\(d\\) dimensions.\n\n\n\nConsider two vectors \\(\\mathbf{u} \\in \\mathbb{R}^d\\) and \\(\\mathbf{v} \\in \\mathbb{R}^d\\). We will use \\(\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^d u_i v_i\\) to denote the inner product of \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\). The \\(\\ell_2\\)-norm of \\(v\\) is given by \\(\\|\\mathbf{v}\\|_2 = \\sqrt{\\mathbf{u} \\cdot \\mathbf{u}}\\).\nMatrix multiplication lives at the heart of deep learning. In fact, deep learning really started to take off when researchers realized that the Graphical Processing Unit (GPU) could be used to perform gradient updates in addition to the matrix multiplication it was designed to do for gaming.\nConsider two matrices: \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times m}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{m \\times n}\\) where \\(d \\neq n\\).\nWe can only multiply two matrices when their inner dimension agrees. Because the number of columns in \\(\\mathbf{A}\\) is the same as the number of rows in \\(\\mathbf{B}\\), we can compute \\(\\mathbf{AB}\\). However, because the number of columns in \\(\\mathbf{B}\\) is not the same as the number of rows in \\(\\mathbf{A}\\), the product is \\(\\mathbf{BA}\\) is not defined.\nWhen we can multiply two matrices, the \\((i,j)\\) entry in \\(\\mathbf{AB}\\) is given by the inner product between the \\(i\\)th row of \\(\\mathbf{A}\\) and the \\(j\\)th column of \\(\\mathbf{B}\\). The resulting dimensions of the matrix product will be the number of rows in \\(\\mathbf{A}\\) by the number of columns in \\(\\mathbf{B}\\).\n\n\n\nIf we have a scalar equation \\(ax = b\\), we can simply solve for \\(x\\) by dividing both sides by \\(a\\). In effect, we are applying the inverse of \\(a\\) to \\(a\\) since \\(\\frac1{a} a =1\\). The same principle applies to matrices. For matrices, the \\(n \\times n\\) identity matrix generalizes the scalar \\(1\\). The identity matrix is denoted by \\(\\mathbf{I}_{n \\times n} \\in \\mathbb{R}^{n \\times n}\\): the on-diagonal entries \\((i,i)\\) are 1 while the off-diagonal entries \\((i,j)\\) for \\(i\\neq j\\) are 0.\nSuppose we have the equation \\(\\mathbf{Ax} = \\mathbf{b}\\) where \\(\\mathbf{A} \\in \\mathbb{R}^{d \\times d}\\), \\(\\mathbf{x} \\in \\mathbb{R}^d\\), and \\(\\mathbf{b} \\in \\mathbb{R}^d\\). (Notice that the inner dimensions of \\(\\mathbf{A}\\) and \\(\\mathbf{x}\\) agree so their multiplication is well-defined, and the resulting vector is the same dimension as \\(\\mathbf{b}\\).)\nIf we want to solve for \\(\\mathbf{x}\\), we can use the same inverse idea. For a matrix \\(\\mathbf{A}\\), we use \\(\\mathbf{A}^{-1}\\) to denote its inverse. The inverse is defined so that \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_{n \\times n}\\) where \\(\\mathbf{I}_{n \\times n}\\) is the identity matrix."
  },
  {
    "objectID": "notes/code.html",
    "href": "notes/code.html",
    "title": "Code for Producing Images in Lecture Notes",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\n\n\nLinear Regression Figures\n\nnp.random.seed(1234) # Seed randomness\n\nn = 10 # Number of observations\nw = 2 # True parameter\nX = np.random.rand(n) # x-values\ny = X.dot(w).T + np.random.normal(size=n) * .2 #y-values\n\n\nplt.scatter(X,y, color='black', label=r'Data: $(x_i, y_i)$')\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\nxaxis = np.arange(0,1,.01)\nplt.plot(xaxis, xaxis*.5, label=r'Line: $f(x) = .5x$', color='red')\nplt.plot(xaxis, xaxis*w, label=r'Line: $f(x) = 2x$', color='green')\nplt.legend()\nplt.title(r'Linear Regression in $\\mathbb{R}^1$')\nplt.savefig('images/regression_1d.pdf')\n\n\n\n\n\n\n\n\n\nplt.xlabel(r'$z$')\nplt.ylabel(r'$\\mathcal{L}(z)$')\nxaxis = np.arange(-1.5,1.5,.001)\nplt.plot(xaxis, xaxis**2, label=r'Squared Loss: $\\mathcal{L}(z)=z^2$', color='blue')\nplt.plot(xaxis, np.abs(xaxis), label=r'Absolute Loss: $\\mathcal{L}(z)=|z|$', color='purple', linestyle='dotted')\nplt.legend()\nplt.title(r'Squared and Absolute Losses')\nplt.savefig('images/regression_losses.pdf')"
  }
]