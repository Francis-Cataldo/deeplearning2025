[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Course Description: In this class, we will discover how data science techniques are deployed at scale. The questions we investigate will include: How do services such as Shazam recognize song clips in seconds? In settings with hundreds of features, how do we find patterns? Given a social network, how can we detect groups? And how can we use vibrations to “see” into the earth? We’ll answer these questions and more by exploring how randomization lets us get away with far fewer resources than we’d otherwise need. Topics include random variables, concentration inequalities, dimensionality reduction, singular value decomposition, spectral graph theory, and approximate linear regression.\nPrerequisites: I will assume familiarity with linear algebra and algorithms. We will write some code in python and I expect you to be familiar with it.\nStructure: We will meet on Monday, Tuesday, Wednesday, and Thursday at 75 Shannon in Room 202. The lecture is from 10am to 12pm and the discussion is from 2 to 3pm. I will hold my office hours directly after the discussion.\nResources: This class uses material from Chris Musco’s phenomenal graduate Algorithmic Machine Learning and Data Science class at NYU Tandon. For each class, I will post my handwritten notes, the python notebook for the demo, and the written material I used to prepare. My goal is for you to focus on learning the material in class without the need to write your own notes.\n\nGrading\nYour grade in the class will be based on the number of points you earn. You will receive an A if you earn 93 or more points, an A- if you earn between 90 and 92 points, a B+ if you earn between 87 and 89 points, etc.\nParticipation and Questions (14 points): Since winter term classes are smaller, let’s take advantage of the opportunity for more engagement. Unless you have a reasonable excuse (e.g. sickness, family emergency), I expect you to attend every lecture and discussion. Whether you are able to attend or not, I expect you to fill out the form linked from the home page to receive credit for engagement (one point per lecture day that you fill it out). Of course, if you are not able to attend in person, you should read the reading before filling out the form.\nProblem Sets (56 points): There will be one problem per class. In order to encourage engagement with the solutions, your problem set grade will be based both on your solutions and your self-grade of your solutions.\nPart 1: Solutions (3 points per homework problem)\nWriting clean and understandable math is an important skill so a component of your homework grade is to typeset your solutions in LaTeX. The homework (and LaTeX source) is posted on the homepage. I suggest working in Overleaf to avoid the hassle of setting up the requisite LaTeX software on your own computer.\nPart 2: Self-grade (1 points per homework problem)\nIn order to encourage engagement with the solutions, you will reflect on what you did well in the homework and what you learned. You should submit the self-grade after comparing your answers to the solutions.\nProject (30 points): The purpose of randomized algorithms is to solve problems more efficiently in practice. In order to practice, you will select a topic we cover in class and implement an algorithm we discussed on a data set of your choosing. You will write a report describing your results and what you learned. You will also give a presentation showcasing your code to the class. You can complete your project as an individual or with a partner.\nLate Policy: I expect all assignments to be turned in on time. If you are unable to turn in an assignment on time, you must email me before the assignment is due to request an extension.\n\n\nHonor Code\nAcademic integrity is an important part of your learning experience. You are welcome to use online material and discuss problems with others but you must explicitly acknowledge the outside resources on the work you submit.\nIf I notice that you have copied someone else’s work without proper attribution (such as code from the internet without a reference link or a solution very close to another student’s without giving credit), I will give you a warning. After the warning, I will subtract 5 points for every violation.\n\n\nAcademic Accommodations\nIf you have a Letter of Accommodation, it is your responsibility to contact me as early in the term as possible. If you do not have a Letter of Accommodation and you believe you are eligible, please reach out to the ADA Coordinators at ada@middlebury.edu."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CSCI 1051: Deep Learning",
    "section": "",
    "text": "Deep learning from theoretical foundations to state-of-the-art applications.\n\n\n\n\nInstructor: R. Teal Witter. Please call me Teal.\nClass Times: We meet Monday, Tuesday, Wednesday, and Thursday in 75 Shannon St Room 202. The lecture is from 10am to noon and the discussion is from 2 to 4pm.\nOffice Hours: I will hold office hours in 75 Shannon St Room 221 from 3 to 4pm.\nParticipation: I expect you to engage in class, ask questions, and make connections. So that I can get a sense of how you’re doing, please fill out this form once per lecture. (You will receive one point per lecture if you answer the form.)\n\n\nAssignments: You will receive one problem per class. I expect you to solve the problem with your group during the discussion (I’ll be there to answer questions). Once you have solved the problem, you should write up your solution on your own. In addition, there will be a project on a lecture topic of your choice.\n\n\n\nAssignment\n\n\nWork Due\n\n\nSelf-grade Due\n\n\n\n\nProblem Set 1\n\n\nFriday 1/10\n\n\nMonday 1/13\n\n\n\n\nProblem Set 2\n\n\nFriday 1/17\n\n\nMonday 1/20\n\n\n\n\nProblem Set 3\n\n\nFriday 1/24\n\n\nMonday 1/27\n\n\n\n\nProject Proposal\n\n\nMonday 1/20\n\n\n\n\n\n\nProject\n\n\nFriday 1/31\n\n\n\n\n\n\n\n\n\n\nClass\n\n\nTopic\n\n\nSlides\n\n\nResources\n\n\n\n\nThe Three-Step Framework\n\n\n\n\nMonday 1/6\n\n\nLinear Regression and Mean Squared Error\n\n\n\n\n\n\n\n\nTuesday 1/7\n\n\nLogistic Regression and Cross Entropy Loss\n\n\n\n\n\n\n\n\nWednesday 1/8\n\n\nGradient Descent and Neural Networks\n\n\n\n\n\n\n\n\nThursday 1/9\n\n\nBack-propagation and Optimization\n\n\n\n\n\n\n\n\nLanguage Generation\n\n\n\n\nMonday 1/13\n\n\nLanguage Embeddings and Contrastive Loss\n\n\n\n\n\n\n\n\nTuesday 1/14\n\n\nTransformers and Positional Encoding\n\n\n\n\n\n\n\n\nWednesday 1/15\n\n\nLarge Language Models\n\n\n\n\n\n\n\n\nThursday 1/16\n\n\nFinetuning\n\n\n\n\n\n\n\n\nImage Generation\n\n\n\n\nTuesday 1/21\n\n\nConvolutional Neural Networks and Image Embeddings\n\n\n\n\n\n\n\n\nWednesday 1/22\n\n\nDiffusion\n\n\n\n\n\n\n\n\nThursday 1/23\n\n\nSchrödinger Bridges\n\n\n\n\n\n\n\n\nAI Safety\n\n\n\n\nMonday 1/27\n\n\nInterpretability\n\n\n\n\n\n\n\n\nTuesday 1/28\n\n\nWatermarking\n\n\n\n\n\n\n\n\nWednesday 1/29\n\n\nProject Preparation\n\n\n\n\n\n\n\n\nThursday 1/30\n\n\nProject Presentations"
  }
]